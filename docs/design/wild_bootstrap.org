#+TITLE: Moment Wild Bootstrap for GMM on Manifolds
#+AUTHOR: ManifoldGMM Maintainers
#+OPTIONS: toc:nil num:nil

* Purpose
Specify the moment wild bootstrap procedure for constructing confidence regions
that live on the parameter manifold \(\mathcal{M}\), rather than in the tangent
space.  This complements the criterion-based approach in
[[file:confidence_regions.org][confidence_regions.org]] by providing
data-driven critical values that adapt to manifold curvature and finite-sample
behaviour.

* Motivation
The GMM inference layer produces a sandwich covariance
\(\Sigma_\xi\) in tangent coordinates and uses the asymptotic
\(\chi^2_{p}\) distribution for Wald tests and confidence regions.  On curved
manifolds with moderate sample sizes, this normal approximation can be
inadequate for two reasons:

1. The retraction chart introduces non-linear distortion that the first-order
   linearization ignores.
2. The \(\chi^2\) critical value is calibrated for Euclidean geometry and does
   not account for curvature of \(\mathcal{M}\).

The wild bootstrap provides an alternative that is:
- Asymptotically valid :: Under standard GMM regularity conditions the
  bootstrap distribution consistently estimates the sampling distribution.
- Finite-sample calibrated :: Critical values adapt to the actual curvature
  and sample size without parametric distributional assumptions.
- Manifold-native :: Replicates are re-estimated on \(\mathcal{M}\) so
  geodesic distances replace Euclidean norms.

* Statistical Setup

** Moment wild bootstrap
The key insight is to resample moment *errors*, not observation pairs
\((y_i, X_i)\).  Given observation-level moments
\(g_i(\theta) \in \mathbb{R}^{\ell}\), the bootstrap sample mean is

\[
  \bar g^*_N(\theta)
  = \frac{1}{n}\sum_{i=1}^n w_i\, g_i(\theta),
\]

where the weights \(w_i\) are drawn independently from a distribution
satisfying \(E[w_i] = 1\) and \(\operatorname{Var}[w_i] = 1\).  Division by
\(n\) (not \(\sum w_i\)) ensures unbiasedness.

This resamples the moment *errors* because, at the population minimizer
\(\theta_0\), the moments \(g_i(\theta_0)\) are centered residuals.  The
weights perturb their contribution to the sample mean, mimicking the sampling
variability of the original estimator.

** Weight distributions
Three distributions are implemented:

- Rademacher (default) :: \(w_i \in \{0, 2\}\) with equal probability, i.e.,
  \(w_i = 1 + \epsilon_i\) where \(\epsilon_i \in \{-1, +1\}\).  Properties:
  \(E[w] = 1\), \(\operatorname{Var}[w] = 1\), \(E[(w-1)^3] = 0\).  Preferred
  on the basis of Davidson & Flachaire (2008): the zero-skewness property
  avoids the noise that third-moment matching introduces in second-moment
  estimation.
- Mammen :: Two-point distribution with \(E[(w-1)^3] = 1\), matching the
  (unknown) skewness of the residuals.  Provides an asymptotic refinement
  under certain conditions but can degrade finite-sample performance
  (Davidson & Flachaire, 2008).
- Exponential :: \(w_i \sim \operatorname{Exp}(1)\).  Equivalent to the
  Bayesian bootstrap of Rubin (1981).  Smooth weights; always positive.

* Geodesic Mahalanobis Distance

For each bootstrap replicate \(b\), we compute

\[
  d^2_b
  = \operatorname{Log}_{\hat\theta}(\hat\theta^*_b)^\top\,
    \Sigma^{-1}\,
    \operatorname{Log}_{\hat\theta}(\hat\theta^*_b),
\]

where:
- \(\operatorname{Log}_{\hat\theta}\) :: Riemannian logarithmic map at the
  base estimate.  Maps the bootstrap replicate back to the tangent space.
- \(\Sigma\) :: Tangent covariance from =GMMResult.tangent_covariance()=.
- The inner product :: Projects the log vector onto the canonical tangent
  basis via \(\xi_j = \langle e_j,\, v \rangle_{\hat\theta}\) using the
  manifold's Riemannian inner product.

The point \(\theta\) belongs to the \((1-\alpha)\) confidence region if

\[
  d^2_\Sigma(\hat\theta, \theta) \le c_{1-\alpha},
\]

where \(c_{1-\alpha}\) is the \((1-\alpha)\) quantile of \(\{d^2_b\}_{b=1}^B\).

When the manifold does not expose a log map, the implementation falls back to
projecting the ambient difference onto the tangent space, which is a
first-order approximation valid in a neighbourhood of \(\hat\theta\).

* Algorithm

The full bootstrap procedure:

1. *Estimate* :: Obtain \(\hat\theta \in \mathcal{M}\) and extract the
   weighting matrix \(W\) evaluated at \(\hat\theta\).
2. *Generate tasks* :: For each replicate \(b = 1, \ldots, B\):
   a. Draw weights \(w^{(b)} = (w_1, \ldots, w_n)\) from the chosen
      distribution using seed \(\text{base\_seed} + b\).
   b. Package a =BootstrapTask= containing the restriction, \(W\),
      \(\hat\theta\) as initial point, and the seed.
3. *Execute tasks* :: Each task (on a worker node):
   a. Generate weights from its seed.
   b. Clone the restriction with =with_weights(w)=.
   c. Run =GMM(..., weighting=FixedWeighting(W)).estimate()=.
   d. Return a lightweight =BootstrapResult= (point, criterion, convergence).
4. *Collect results* :: Gather all =BootstrapResult= objects.
5. *Compute distances* :: For each converged replicate, compute \(d^2_b\) via
   the log map and tangent covariance.
6. *Critical value* :: The \((1-\alpha)\) quantile of \(\{d^2_b\}\) is the
   critical value for the confidence region.

* Computational Design

** Serializable tasks
Each =BootstrapTask= is self-contained and pickle-serializable, suitable for
dispatch via Ray, Dask, multiprocessing, or any task queue.  The
=to_bytes()= / =from_bytes()= interface uses standard =pickle= with a
=cloudpickle= fallback for JAX closure serialization.

** Data-size considerations
Each serialized task carries one copy of the dataset
(\(n \times p \times 8\) bytes).  This is acceptable for
\(n \approx 10^4\)--\(10^5\).  For larger datasets, broadcast the data once
per node (e.g., =ray.put= or =dask.scatter=) and use lightweight task stubs.
A =scatter_tasks()= API is deferred until needed.

** Sequential mode
=MomentWildBootstrap.run_sequential()= loops over tasks in the current
process, intended for debugging and small-scale testing.

* Relationship to Criterion-Based Regions

The criterion-based approach in [[file:confidence_regions.org][confidence_regions.org]] traces the
boundary of the sublevel set \(\{J_N(\theta) - J_N(\hat\theta) \le c\}\) via
directional root-finding.  This is useful for:
- Visualization :: Provides an explicit boundary in the parameter space.
- Marginal intervals :: Projects the boundary to individual parameter axes.

The bootstrap provides:
- Calibrated coverage :: Critical values adapt to finite-sample behaviour and
  manifold curvature.
- Membership test :: Efficient point-in-region test without boundary tracing.

The two approaches complement rather than replace each other.  The bootstrap
critical value can also be used as the threshold \(c\) in the criterion-based
boundary search, combining the best of both.

* References
- Davidson, R. & Flachaire, E. (2008). The wild bootstrap, tamed at last.
  /Journal of Econometrics/, 146(1), 162--169.
- Bhattacharya, R. & Patrangenaru, V. (2005). Large sample theory of
  intrinsic and extrinsic sample means on manifolds.  /Annals of Statistics/,
  33(1), 1--29.
- Rubin, D. B. (1981). The Bayesian bootstrap.  /Annals of Statistics/,
  9(1), 130--134.
