#+TITLE: Autodiff & Jacobians
#+AUTHOR: ManifoldGMM Maintainers
#+OPTIONS: toc:nil num:nil

* Goal
Establish a backend-agnostic strategy for computing Jacobians of vector-valued
functions defined on Riemannian manifolds. The first milestone focuses on a
JAX-based implementation that wraps =pymanopt= manifolds without forking upstream.

* Context
- Target functions accept a =ManifoldPoint= and return values in ℝ^ℓ (or PyTrees
  thereof) at a single evaluation point.
- Jacobians are linear maps D f(θ): T_θ M → ℝ^ℓ exposed via =matvec= and
  =T_matvec=, always projected to the tangent space given by the manifold.
- Future backends (e.g., Autograd) should reuse the same operator interface.

* Milestone A: JAX Backend
1. Implement a helper that, given a vector-valued function f and a
   =ManifoldPoint= θ, returns a Jacobian operator backed by =jax.linearize=.
2. Ensure tangent projections use the manifold’s =proj= routine so that off-manifold
   perturbations are corrected automatically.
3. Document how batching could be added later via =jax.vmap=, noting current scope
   is single-θ evaluations.

* Implemented Components (WIP)
- =manifoldgmm.autodiff.jax_backend.jacobian_operator=: builds a linear operator
  with =matvec= and =T_matvec= closures using =jax.linearize= and
  =jax.linear_transpose=.
- =manifoldgmm.geometry.Manifold= and =ManifoldPoint=: minimal wrappers for
  projection routines that keep θ on the manifold and define Π_θ.
- Tests cover Euclidean examples; extend to curved manifolds once we integrate
  concrete =pymanopt= manifolds.

* Open Questions
- Should Jacobians cache factorizations (e.g., QR) for repeated matvec calls?
- How should we expose structured Jacobians when f outputs PyTrees?
- Do we need a unified interface for numerical finite-difference fallbacks?

* Notes on pymanopt Backend Integration
- The vendored submodule at =third_party/pymanopt= exposes =JaxBackend=
  (see =third_party/pymanopt/src/pymanopt/autodiff/backends/_jax.py=) which
  currently provides gradients and Hessian-vector products only.
- Extending pymanopt will likely involve adding a Jacobian factory to the
  backend base class and wiring a =jax.linearize=/=jax.linear_transpose=
  implementation analogous to our helper.
