#+TITLE: Autodiff & Jacobians
#+AUTHOR: ManifoldGMM Maintainers
#+OPTIONS: toc:nil num:nil

* Goal
Establish a backend-agnostic strategy for computing Jacobians of vector-valued
functions defined on Riemannian manifolds. The first milestone focuses on a
JAX-based implementation that wraps =pymanopt= manifolds without forking upstream.

* Context
- Target functions accept a =ManifoldPoint= and return values in ℝ^ℓ (or PyTrees
  thereof) at a single evaluation point.
- Jacobians are linear maps D f(θ): T_θ M → ℝ^ℓ exposed via =matvec= and
  =T_matvec=, always projected to the tangent space given by the manifold.
- Future backends (e.g., Autograd) should reuse the same operator interface.

* Milestone A: JAX Backend
1. Implement a helper that, given a vector-valued function f and a
   =ManifoldPoint= θ, returns a Jacobian operator backed by =jax.linearize=.
2. Ensure tangent projections use the manifold’s =proj= routine so that off-manifold
   perturbations are corrected automatically.
3. Document how batching could be added later via =jax.vmap=, noting current scope
   is single-θ evaluations.

* Implemented Components (WIP)
- =manifoldgmm.autodiff.jax_backend.jacobian_operator=: builds a linear operator
  with =matvec= and =T_matvec= closures using =jax.linearize= and
  =jax.linear_transpose=.
- =manifoldgmm.autodiff.jacobian_from_pymanopt=: convenience wrapper that
  accepts a ``pymanopt`` manifold plus an ambient function and returns the
  same Jacobian operator for downstream code.
- =manifoldgmm.geometry.Manifold= and =ManifoldPoint=: minimal wrappers for
  projection routines that keep θ on the manifold and define Π_θ.
- Tests cover Euclidean examples; extend to curved manifolds once we integrate
  concrete =pymanopt= manifolds.

* Usage Example
#+begin_src python
import jax.numpy as jnp

from pymanopt.manifolds.psd import PSDFixedRank
from manifoldgmm.autodiff import jacobian_from_pymanopt

manifold = PSDFixedRank(n=3, k=2)
theta = manifold.random_point()

def vector_function(y):
    gram = y.T @ y
    return jnp.stack([jnp.trace(gram), jnp.linalg.det(gram + jnp.eye(gram.shape[0]))])

jac = jacobian_from_pymanopt(vector_function, manifold, theta)
direction = manifold.projection(theta, np.random.randn(*theta.shape))
jacobian_action = jac.matvec(direction)
#+end_src

* Open Questions
- Should Jacobians cache factorizations (e.g., QR) for repeated matvec calls?
- How should we expose structured Jacobians when f outputs PyTrees?
- Do we need a unified interface for numerical finite-difference fallbacks?

* Notes on pymanopt Backend Integration
- The vendored submodule at =third_party/pymanopt= exposes =JaxBackend=
  (see =third_party/pymanopt/src/pymanopt/autodiff/backends/_jax.py=) which
  currently provides gradients and Hessian-vector products only.
- Extending pymanopt will likely involve adding a Jacobian factory to the
  backend base class and wiring a =jax.linearize=/=jax.linear_transpose=
  implementation analogous to our helper.
