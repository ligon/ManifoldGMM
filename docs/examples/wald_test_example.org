#+TITLE: Wald Test on Manifolds
#+AUTHOR: ManifoldGMM Maintainers
#+PROPERTY: header-args:python :session wald_test_example :results output :exports both :tangle wald_test_example.py

* Introduction
This example demonstrates how to perform a Wald test for hypothesis testing on parameters estimated via Generalized Method of Moments (GMM) on a Riemannian manifold.

We verify the test using a synthetic example on the circle manifold \(\mathbb{S}^1\).

* Mathematical Background
Consider a parameter \(\theta \in \mathcal{M}\) estimated by \(\hat{\theta}\).
We wish to test the null hypothesis:
\[ H_0: h(\theta) = 0 \]
where \(h: \mathcal{M} \to \mathbb{R}^q\) is a differentiable constraint function.

The Wald statistic is defined as:
\[ W = h(\hat{\theta})^⊡ \left( H \Sigma H^⊡ \right)^{-1} h(\hat{\theta}) \]
where:
- \(H = D h(\hat{\theta})\) is the Jacobian of the constraint with respect to the tangent space at \(\hat{\theta}\).
- \(\Sigma\) is the estimated covariance matrix of \(\hat{\theta}\) in the tangent space.

Under \(H_0\), \(W \xrightarrow{d} \chi^2_q\).

* Example: Mean Direction on the Circle
We generate data from a von Mises-like distribution (projected Gaussian) centered at \(\mu_0 = (1, 0)\).
We test the hypothesis that the mean direction has a zero y-component: \(\theta_y = 0\).

** Setup
#+begin_src python
  import jax.numpy as jnp
  import numpy as np
  from manifoldgmm import GMM, Manifold, MomentRestriction, ManifoldPoint
  from pymanopt.manifolds import Sphere, Euclidean

  # Define the moment condition: Orthogonality to the mean direction
  ROT90 = jnp.array([[0.0, -1.0], [1.0, 0.0]], dtype=jnp.float64)

  def gi_jax(theta, observation):
      theta_perp = ROT90 @ theta
      return jnp.array([jnp.dot(theta_perp, observation)], dtype=jnp.float64)

  # Generate synthetic data
  rng = np.random.default_rng(42)
  n_obs = 100
  # True mean at (1, 0)
  data_raw = rng.normal(loc=[1.0, 0.0], scale=0.5, size=(n_obs, 2))
  norms = np.linalg.norm(data_raw, axis=1, keepdims=True)
  data = data_raw / norms

  # Define Manifold and Restriction
  manifold = Manifold.from_pymanopt(Sphere(2))
  restriction = MomentRestriction(
      gi_jax=gi_jax,
      data=jnp.array(data),
      manifold=manifold,
      backend="jax",
      parameter_labels=["x", "y"]
  )
#+end_src

** Estimation
#+begin_src python
  gmm = GMM(restriction, initial_point=jnp.array([0.0, 1.0])) # Start away from truth
  result = gmm.estimate()
  print(f"Estimated theta: {result.theta.value}")
#+end_src

** Wald Test
We define the constraint \(h(\theta) = \theta_y\).
#+begin_src python
  def constraint(theta_point):
      return theta_point.value[1]

  wald = result.wald_test(constraint, q=1)
  print(f"Wald Statistic: {wald.statistic:.4f}")
  print(f"P-value: {wald.p_value:.4f}")
  print(f"Degrees of Freedom: {wald.degrees_of_freedom}")
#+end_src

* Interpretation
If the p-value is greater than the significance level (e.g., 0.05), we fail to reject the null hypothesis. Since the true mean is \((1, 0)\), we expect to fail to reject \(H_0: \theta_y = 0\).

* Power Analysis (H1)
Now let's test a false hypothesis. Suppose we test if the mean is at 45 degrees: \(\theta_x = \theta_y\).
This corresponds to \(h(\theta) = \theta_x - \theta_y = 0\).
Since \(\hat{\theta} \approx (1, 0)\), \(\hat{\theta}_x - \hat{\theta}_y \approx 1 \neq 0\). We expect to reject.

#+begin_src python
  def constraint_h1(theta_point):
      val = theta_point.value
      return val[0] - val[1]

  wald_h1 = result.wald_test(constraint_h1, q=1)
  print(f"H1 Wald Statistic: {wald_h1.statistic:.4f}")
  print(f"H1 P-value: {wald_h1.p_value:.4e}")
#+end_src

* Monte Carlo Simulation
We can systematically verify the test's size (rejection rate under \(H_0\)) and power (rejection rate under \(H_1\)).

** Size Simulation
We repeat the estimation 100 times under \(H_0\) (\(\mu_0 = (1, 0)\)) and count rejections of \(\theta_y = 0\) at \(\alpha = 0.05\).

#+begin_src python
  n_reps = 100
  rejections_h0 = 0
  alpha = 0.05

  for _ in range(n_reps):
      # Generate data under H0
      data_rep = rng.normal(loc=[1.0, 0.0], scale=0.5, size=(n_obs, 2))
      data_rep /= np.linalg.norm(data_rep, axis=1, keepdims=True)
      
      restriction_rep = MomentRestriction(
          gi_jax=gi_jax,
          data=jnp.array(data_rep),
          manifold=manifold,
          backend="jax"
      )
      gmm_rep = GMM(restriction_rep, initial_point=jnp.array([1.0, 0.0]))
      res_rep = gmm_rep.estimate(verbose=0)
      
      if res_rep.wald_test(constraint, q=1).p_value < alpha:
          rejections_h0 += 1

  print(f"Rejection rate under H0 (Size): {rejections_h0 / n_reps:.2f}")
  # Expect roughly 0.05
#+end_src

** Power Simulation
We repeat the estimation 20 times under \(H_1\) (\(\mu_1\) rotated by 0.5 rad) and count rejections of \(\theta_y = 0\).

#+begin_src python
  n_reps_h1 = 20
  rejections_h1 = 0
  
  angle = 0.5
  mu_1 = np.array([np.cos(angle), np.sin(angle)])

  for _ in range(n_reps_h1):
      # Generate data under H1
      data_rep = rng.normal(loc=mu_1, scale=0.5, size=(n_obs, 2))
      data_rep /= np.linalg.norm(data_rep, axis=1, keepdims=True)
      
      restriction_rep = MomentRestriction(
          gi_jax=gi_jax,
          data=jnp.array(data_rep),
          manifold=manifold,
          backend="jax"
      )
      gmm_rep = GMM(restriction_rep, initial_point=jnp.array([1.0, 0.0]))
      res_rep = gmm_rep.estimate(verbose=0)
      
      if res_rep.wald_test(constraint, q=1).p_value < alpha:
          rejections_h1 += 1

  print(f"Rejection rate under H1 (Power): {rejections_h1 / n_reps_h1:.2f}")
  # Expect high power (close to 1.0)
#+end_src

** Power Curve Comparison
We now construct a power curve by varying the deviation of \(\theta_y\) from 0. We compare the Manifold Wald test against a standard Euclidean Wald test (using \(\mathbb{R}^2\) geometry and the moment \(g(\theta, x) = x - \theta\)).

#+begin_src python
  import matplotlib.pyplot as plt

  angles = np.linspace(0, 0.5, 11) # From 0 to ~30 degrees
  n_reps_curve = 50
  
  power_manifold = []
  power_euclidean = []
  
  # Euclidean restriction setup
  manifold_euc = Manifold.from_pymanopt(Euclidean(2))
  def gi_euc(theta, observation):
      return observation - theta
      
  for phi in angles:
      mu_true = np.array([np.cos(phi), np.sin(phi)])
      
      rej_man = 0
      rej_euc = 0
      
      for _ in range(n_reps_curve):
          data_rep = rng.normal(loc=mu_true, scale=0.5, size=(n_obs, 2))
          data_rep /= np.linalg.norm(data_rep, axis=1, keepdims=True)
          data_jax = jnp.array(data_rep)
          
          # Manifold GMM
          res_man = GMM(
              MomentRestriction(gi_jax=gi_jax, data=data_jax, manifold=manifold, backend="jax"),
              initial_point=jnp.array([1.0, 0.0])
          ).estimate(verbose=0)
          
          if res_man.wald_test(constraint, q=1).p_value < alpha:
              rej_man += 1
              
          # Euclidean GMM
          res_euc = GMM(
              MomentRestriction(gi_jax=gi_euc, data=data_jax, manifold=manifold_euc, backend="jax"),
              initial_point=jnp.array([1.0, 0.0])
          ).estimate(verbose=0)
          
          if res_euc.wald_test(constraint, q=1).p_value < alpha:
              rej_euc += 1
              
      power_manifold.append(rej_man / n_reps_curve)
      power_euclidean.append(rej_euc / n_reps_curve)

  # Plotting
  plt.figure(figsize=(8, 5))
  plt.plot(angles, power_manifold, 'o-', label='Manifold GMM')
  plt.plot(angles, power_euclidean, 's--', label='Euclidean GMM')
  plt.xlabel('Deviation Angle (radians)')
  plt.ylabel('Power (Rejection Rate)')
  plt.title('Power Curve: Manifold vs Euclidean Wald Test')
  plt.legend()
  plt.grid(True)
  plt.savefig('power_curve.png')
  print("Power curve saved to power_curve.png")
#+end_src

The Euclidean estimator treats the data as living in \(\mathbb{R}^2\). Since the data lies on the unit circle, the sample mean will be strictly inside the circle ("shrinkage"). This bias towards the origin reduces the magnitude of the estimated \(\hat{\theta}_y\) component compared to the Manifold estimator which stays on the circle. Since we are testing \(H_0: \theta_y = 0\), this shrinkage reduces the test statistic, leading to lower power for the Euclidean test compared to the Manifold test.