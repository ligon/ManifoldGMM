#+title: Inference on Fixed-Rank PSD Matrices: Manifold GMM as Factor Extraction
#+author: ManifoldGMM Maintainers
#+property: header-args:python :session psd_example :results output :exports both

* Introduction

This example compares inference for a covariance matrix under two approaches:
1. *Unconstrained (Euclidean):* Estimate all unique elements of a symmetric matrix.
2. *Constrained (Manifold):* Estimate a matrix constrained to have fixed low rank.

We show that the manifold approach acts as an implicit factor analyzer,
separating low-rank signal from idiosyncratic noise---a property the
Euclidean approach lacks.

* The Factor Model

Consider a single-factor model for \(p\)-dimensional observations:
\[
x_i = z_i \, v + \varepsilon_i
\]
where:
- \(z_i \sim N(0, 1)\) :: scalar latent factor
- \(v \in \mathbb{R}^p\) :: factor loading vector
- \(\varepsilon_i \sim N(0, \sigma^2 I_p)\) :: idiosyncratic noise

The covariance of \(x_i\) decomposes as:
\[
\mathrm{Cov}(x) = v v^\top + \sigma^2 I = \underbrace{A}_{\text{rank-1 signal}} + \underbrace{\sigma^2 I}_{\text{noise}}
\]

** What Does Each Estimator Target?

Both estimators use the moment condition \(E[x_i x_i^\top - \Theta] = 0\),
which identifies \(\Theta = E[x_i x_i^\top] = \mathrm{Cov}(x)\).

- Euclidean GMM :: estimates \(\hat\Theta \to v v^\top + \sigma^2 I\) (the full covariance).
- Manifold GMM :: (rank-1 constraint) can only fit a rank-1 matrix, so it extracts
  \(\hat\Theta \to v v^\top\) (the signal component).

The manifold constraint acts as a structural filter, separating signal from noise.

* Setup: Testing a Factor Loading

We set \(p = 3\), \(v = (1, v_1, 0)^\top\), and test:
\[
H_0: A_{22} = v_1^2 = 0 \quad \text{(equivalently, } v_1 = 0 \text{)}
\]

Under the null (\(v_1 = 0\)), the true signal matrix \(A = v v^\top\) has \(A_{22} = 0\).
However, the full covariance has \(\mathrm{Cov}(x)_{22} = 0 + \sigma^2 = \sigma^2 > 0\).

This creates a fundamental difference:
- The Euclidean estimator converges to \(\sigma^2\), not 0, even under \(H_0\).
- The Manifold estimator converges to 0 (extracting only the rank-1 signal).

* Single Example

#+begin_src python :results output
  import jax.numpy as jnp
  import numpy as np
  from manifoldgmm import GMM, Manifold, MomentRestriction
  from pymanopt.manifolds import PSDFixedRank, Euclidean

  # Parameters
  n_obs = 100
  noise_scale = 0.1  # sigma
  v_true = np.array([[1.0], [0.0], [0.0]])  # Null is TRUE: v_1 = 0

  # Data generating process: x = z * v' + noise
  rng = np.random.default_rng(42)
  z = rng.normal(size=(n_obs, 1))
  data = z @ v_true.T + rng.normal(scale=noise_scale, size=(n_obs, 3))
  data_jax = jnp.array(data)

  # True matrices
  A_true = v_true @ v_true.T  # Signal (rank-1)
  Cov_true = A_true + noise_scale**2 * np.eye(3)  # Full covariance

  print("Signal matrix A = v v':")
  print(A_true)
  print(f"\nTrue A[1,1] = {A_true[1,1]} (this is what H0 tests)")
  print(f"\nFull Cov(x)[1,1] = A[1,1] + sigma^2 = {Cov_true[1,1]}")
#+end_src

** Moment Functions

#+begin_src python :results output
  # Manifold: PSD(3, 1) parameterized by factor Y (3x1), A = Y Y'
  manifold_man = Manifold.from_pymanopt(PSDFixedRank(3, 1))

  def gi_man(Y, x):
      A = Y @ Y.T
      diff = jnp.outer(x, x) - A
      return diff[jnp.triu_indices(3)]

  def constraint_man(theta_point):
      Y = theta_point.value
      return (Y @ Y.T)[1, 1]

  # Euclidean: R^6 parameterized by vech(A)
  manifold_euc = Manifold.from_pymanopt(Euclidean(6))

  def gi_euc(theta, x):
      A = jnp.zeros((3, 3))
      idx = jnp.triu_indices(3)
      A = A.at[idx].set(theta)
      A = A + A.T - jnp.diag(jnp.diag(A))
      diff = jnp.outer(x, x) - A
      return diff[idx]

  def constraint_euc(theta_point):
      return theta_point.value[3]  # A[1,1] in vech ordering
#+end_src

** Estimation and Testing

#+begin_src python :results output
  # Estimate both models
  res_man = GMM(
      MomentRestriction(gi_jax=gi_man, data=data_jax,
                        manifold=manifold_man, backend="jax"),
      initial_point=np.array([[1.0], [0.0], [0.0]]),
      weighting=np.eye(6),
  ).estimate(verbose=0)

  res_euc = GMM(
      MomentRestriction(gi_jax=gi_euc, data=data_jax,
                        manifold=manifold_euc, backend="jax"),
      initial_point=np.array([1.0, 0.0, 0.0, 1.0, 0.0, 1.0]),
      weighting=np.eye(6),
  ).estimate(verbose=0)

  # Extract estimates
  Y_hat = res_man.theta.value
  A_hat_man = Y_hat @ Y_hat.T
  A_hat_euc = res_euc.theta.value

  print("=== Estimates of A[1,1] ===")
  print(f"True A[1,1] (signal):     {A_true[1,1]:.4f}")
  print(f"Manifold estimate:        {A_hat_man[1,1]:.4f}")
  print(f"Euclidean estimate:       {A_hat_euc[3]:.4f}")
  print(f"(Euclidean targets Cov[1,1] = {Cov_true[1,1]:.4f})")

  # Wald tests
  wald_man = res_man.wald_test(constraint_man, q=1)
  wald_euc = res_euc.wald_test(constraint_euc, q=1)

  print(f"\n=== Wald Tests for H0: A[1,1] = 0 ===")
  print(f"Manifold: W = {wald_man.statistic:6.2f}, p = {wald_man.p_value:.4f}")
  print(f"Euclidean: W = {wald_euc.statistic:6.2f}, p = {wald_euc.p_value:.4f}")
#+end_src

* Power Curve Simulation

We vary the effect size \(v_1\) from 0 (null true) to 0.3 and compute
rejection rates for both tests at \(\alpha = 0.05\).

#+caption: Power curves for Wald tests on \(H_0: A_{22} = 0\).  The Euclidean test rejects 100% even under the null because it targets \(\mathrm{Cov}(x)_{22} = \sigma^2\), not the signal \(A_{22}\).  The Manifold test has correct size and increasing power as \(v_1\) departs from zero.
#+name: fig:power-curve
[[file:psd_power_curve.png]]

The figure above shows the key result:

- At \(v_1 = 0\) (null is true) :: Manifold rejection rate \(\approx 0\%\); Euclidean \(\approx 100\%\).
- As \(v_1\) increases :: Manifold power rises smoothly to 100%.

The Euclidean test's "perfect power" is illusory---it reflects size inflation,
not sensitivity to the alternative.

** Simulation Code                                                 :noexport:

The code below generates the power curve (run interactively or tangle to execute):

#+begin_src python :results output :exports code :eval never-export
  def run_power_simulation(effect_sizes, n_obs=30, n_reps=100,
                           noise_scale=0.1, alpha=0.05, seed=42):
      """Monte Carlo power simulation."""
      rng = np.random.default_rng(seed)
      results = {'effect': [], 'manifold': [], 'euclidean': []}

      for v1 in effect_sizes:
          v = np.array([[1.0], [v1], [0.0]])
          rej_man, rej_euc = 0, 0

          for _ in range(n_reps):
              z = rng.normal(size=(n_obs, 1))
              x = z @ v.T + rng.normal(scale=noise_scale, size=(n_obs, 3))
              x_jax = jnp.array(x)

              try:
                  r_man = GMM(
                      MomentRestriction(gi_jax=gi_man, data=x_jax,
                                        manifold=manifold_man, backend="jax"),
                      initial_point=np.array([[1.0], [0.0], [0.0]]),
                      weighting=np.eye(6),
                  ).estimate(verbose=0)
                  if r_man.wald_test(constraint_man, q=1).p_value < alpha:
                      rej_man += 1
              except Exception:
                  pass

              try:
                  r_euc = GMM(
                      MomentRestriction(gi_jax=gi_euc, data=x_jax,
                                        manifold=manifold_euc, backend="jax"),
                      initial_point=np.array([1.0, 0.0, 0.0, 1.0, 0.0, 1.0]),
                      weighting=np.eye(6),
                  ).estimate(verbose=0)
                  if r_euc.wald_test(constraint_euc, q=1).p_value < alpha:
                      rej_euc += 1
              except Exception:
                  pass

          results['effect'].append(v1)
          results['manifold'].append(rej_man / n_reps)
          results['euclidean'].append(rej_euc / n_reps)
          print(f"v1={v1:.2f}: Manifold={rej_man/n_reps:.2f}, Euclidean={rej_euc/n_reps:.2f}")

      return results

  effect_sizes = [0.0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]
  print("Running power simulation...")
  power_results = run_power_simulation(effect_sizes, n_reps=50)
#+end_src

** Plotting Code                                                   :noexport:

#+begin_src python :results file :file psd_power_curve.png :exports code :eval never-export
  import matplotlib.pyplot as plt

  fig, ax = plt.subplots(figsize=(8, 5))

  ax.plot(power_results['effect'], power_results['manifold'],
          'o-', label='Manifold (rank-1)', linewidth=2, markersize=8, color='#2E86AB')
  ax.plot(power_results['effect'], power_results['euclidean'],
          's--', label='Euclidean (unconstrained)', linewidth=2, markersize=8, color='#E94F37')

  ax.axhline(y=0.05, color='gray', linestyle=':', linewidth=1, label=r'$\alpha$ = 0.05')
  ax.axvline(x=0.0, color='gray', linestyle=':', linewidth=1, alpha=0.5)

  ax.set_xlabel(r'Effect size ($v_1$)', fontsize=12)
  ax.set_ylabel('Rejection rate', fontsize=12)
  ax.set_title(r'Wald Test Power: Manifold vs Euclidean' + '\n' + r'($H_0: A_{22} = 0$)', fontsize=14)
  ax.legend(loc='right', fontsize=10)
  ax.set_ylim(-0.05, 1.05)
  ax.grid(True, alpha=0.3)

  ax.annotate('Euclidean has\ninflated size!',
              xy=(0.02, 1.0), xytext=(0.12, 0.80),
              fontsize=10, arrowprops=dict(arrowstyle='->', color='#E94F37'),
              color='#E94F37')

  ax.annotate('Manifold has\ncorrect size',
              xy=(0.0, 0.05), xytext=(0.10, 0.25),
              fontsize=10, arrowprops=dict(arrowstyle='->', color='#2E86AB'),
              color='#2E86AB')

  plt.tight_layout()
  plt.savefig('psd_power_curve.png', dpi=150)
  print('psd_power_curve.png')
#+end_src

* Discussion

** The Size Problem with Euclidean Inference

At \(v_1 = 0\) (null is true), the Euclidean test rejects nearly 100% of the time.
This is not a bug---it reflects a fundamental mismatch:

- The null hypothesis is \(H_0: A_{22} = 0\) where \(A = v v^\top\) is the signal.
- But the Euclidean estimator targets \(\mathrm{Cov}(x)_{22} = \sigma^2 \neq 0\).

The estimator is consistent for the wrong quantity.  Even with infinite data,
it converges to \(\sigma^2\), not 0.  The Wald test correctly detects that
\(\hat\theta \neq 0\) with high precision.

** Manifold GMM as Factor Extraction

The rank-1 constraint forces the estimator to find the best rank-1
approximation to the sample covariance.  Since the noise is isotropic (\(\sigma^2 I\)),
it contributes equally in all directions and gets "filtered out" by the
low-rank projection.

This is analogous to principal component analysis or factor analysis:
the manifold constraint extracts the dominant factor structure while
ignoring idiosyncratic variation.

** When to Use Manifold Constraints

Manifold-constrained GMM is appropriate when:

- Structural knowledge :: You know the parameter has low-rank or other
  geometric structure (e.g., covariance matrices in factor models,
  rotation matrices, correlation matrices).

- Robustness to noise :: The constraint acts as regularization,
  preventing the estimator from fitting noise.

- Efficiency :: Fewer parameters means lower variance for the
  parameters you care about.

The Euclidean approach is appropriate when:
- You want to estimate the full covariance (including noise variance).
- No structural constraints are known or desired.

* Conclusion

This example demonstrates that manifold constraints in GMM are not just
about efficiency---they fundamentally change what the estimator targets.
For factor models, the rank constraint extracts signal from noise,
enabling valid inference on the factor structure even in the presence
of idiosyncratic variation.
