#+TITLE: GMM on a Product Manifold: Gaussian Location & Covariance
#+AUTHOR: ManifoldGMM Maintainers
#+OPTIONS: toc:nil num:nil
#+PROPERTY: header-args:python :exports code :noweb yes

* Motivation

Ordinary GMM estimation of multivariate Gaussian models treats the location
parameter in Euclidean space and the covariance matrix in the cone of symmetric
positive definite matrices.  When moments are estimated naively, sampling noise
can produce covariance estimates that are indefinite or violate the manifold
structure.  Instead, we place the parameters on a *product manifold*
\[
\mathbb{R}^2 \times \mathcal{S}_{++}^{2},
\]
where \(\mathbb{R}^2\) captures the mean vector and \(\mathcal{S}_{++}^{2}\) denotes the space of \(2\times 2\)
PSD matrices with the canonical affine-invariant metric.

This example sketches how to express the corresponding GMM problem using
~MomentRestriction~ with a JAX backend, and optimise it via the product manifold
support available in =pymanopt=.

* Data and Moments

Let \(x_i \in \mathbb{R}^2\) be i.i.d. draws from a centred Gaussian with true mean
\(\mu_\star\) and covariance \(\Sigma_\star\).  

The first moments (2) are equal to \(\mu_*\), the second (3), because of symmetry) equal to \(\Sigma_*\), and the third equal to zero.
We stack the per-observation moments as
\[
g_i(\mu, \Sigma) =\begin{bmatrix}
x_i - \mu \\
\operatorname{vec}_s\big((x_i - \mu)(x_i - \mu)^{\top} - \Sigma\big)\\
(x_i-\mu)\otimes(x_i-\mu)\otimes(x_i-\mu)
\end{bmatrix}
\]
where \(\operatorname{vec}_s(\cdot)\) vectorizes the symmetric matrix using the
upper-triangular entries.  The moment restriction is satisfied when
\(\mathbb{E}[g_i(\mu_\star, \Sigma_\star)] = 0\).

* Product Manifold Structure

The optimization takes place on the product manifold

- ~Euclidean(2)~ for the mean \(\mu\)
- ~SymmetricPositiveDefinite(2)~ (SPD) for the covariance \(\Sigma\)

=pymanopt= exposes this manifold via ~Product((Euclidean(2), SPD(2)))~.
~MomentRestriction~ can accept the same product manifold by instantiating both
components separately and returning a ~ManifoldPoint~ that wraps the pair
~(\mu, \Sigma)~.

#+name: gaussian-imports
#+begin_src python :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
"""Gaussian location/covariance GMM example (tangled from docs)."""

from __future__ import annotations

from itertools import combinations_with_replacement

import jax.numpy as jnp
import numpy as np
from datamat import DataMat, DataVec
from manifoldgmm import Manifold, MomentRestriction
from pymanopt import Problem
from pymanopt import function as pymanopt_function
from pymanopt.manifolds import Euclidean, Product, SymmetricPositiveDefinite
from pymanopt.optimizers import TrustRegions

#+end_src

* DataMat-first Moment Restriction

Users can implement per-observation moments purely with =DataMat= objects.
~MomentRestriction.from_datamat~ converts these moments into a backend-friendly
function (NumPy or JAX) while preserving the richer metadata for inspection.

** Data Generation
For this experiment we want some data drawn from a multivariate normal distribution.  We'll pretend we don't know the mean or variance, and estimate those.   After estimation we'll peek behind the curtain and compare our estimates with the true values.

#+name: gaussian-data
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance

# Default random variables are distributed standard normal
X = DataMat.random((512, 2), rng=67, columns=["x1", "x2"], colnames="p", idxnames="i")

S = DataMat(
    [[1.0, 0.4], [0.4, 1.1]], index=X.columns, columns=X.columns
)  # Sqrt of variance matrix

mu = DataVec([0.3, -0.2], index=X.columns)

X = X @ S + mu  # Transformed random deviates

Sigma = S @ S.T
#+end_src
** Defining Moment Restriction
We need to define a function which satisfies \(\mathbb{E}g_i(\theta)=0\).  And to use automatic differentiation we need a "jax" version.   The function ``gi`` shows the observation-level logic using pure DataVec/DataMat operations; for the JAX backend we supply ``gi_jax`` so
pymanopt can differentiate the objective automatically. (Future work may bridge the two automatically as described in [[file:../design/feature_requests.org][feature_requests.org]].)

#+name: gaussian-moments
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
useful_3rd_moments = list(combinations_with_replacement([0, 1], 3))


def gi(theta: tuple[DataVec, DataMat], x: DataVec) -> DataVec:
    mu, sigma = theta
    e = x - mu
    e2 = e.outer(e) - sigma
    e3 = e.kron(e).kron(e)  # 3rd central moments = 0
    return e.concat([e2.triu().vec, e3.loc[useful_3rd_moments]])


def gi_jax(theta: tuple[jnp.ndarray, jnp.ndarray], x: jnp.ndarray) -> jnp.ndarray:
    mu, sigma = theta
    e = x - mu
    e2 = e[:, None] @ e[None, :] - sigma
    e3 = jnp.kron(e, jnp.kron(e, e))

    dim = x.shape[0]
    triple_idx = jnp.array(useful_3rd_moments)
    flatten_idx = triple_idx @ jnp.array([dim * dim, dim, 1])

    return jnp.concatenate([e, e2[jnp.triu_indices(dim)], e3[flatten_idx]])

#+end_src

Internally ~restriction~ builds a JAX-compatible representation for optimization,
while methods such as ~g_bar~ and ~omega_hat~ still return =DataMat= objects with
the original labels. 

We next build a ~MomentRestriction~ object which describes a function of parameters which is equal to zero in expected value (this is the =gi= or =gi_jax= function above).  We're interested in evaluating this function on a particular manifold.  So: below we define a manifold =geometry=, and build =restriction=.

#+name: gaussian-momentrestriction
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance

geometry = {}
restriction = {}


geometry["euclidean"] = Manifold.from_pymanopt(Product((Euclidean(2), Euclidean(2, 2))))

restriction["euclidean"] = MomentRestriction(
    gi_jax=gi_jax, data=X.to_jax().values, manifold=geometry["euclidean"], backend="jax"
)

geometry["product"] = Manifold.from_pymanopt(
    Product((Euclidean(2), SymmetricPositiveDefinite(2)))
)

restriction["product"] = MomentRestriction(
    gi_jax=gi_jax, data=X.to_jax().values, manifold=geometry["product"], backend="jax"
) 

#+end_src

** Estimation
There's a more clever way to to this!  But for now treat this as a non-linear least squares problem on our manifold of choice.  The function ~cost~ is defined as a JAX object that returns the inner-product of the moments.

#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
costs = {}


@pymanopt_function.jax(geometry["euclidean"].data)
def cost(mu, sigma):
    point = (mu, sigma)
    moments = restriction["euclidean"].g_bar(point)
    return jnp.dot(moments, moments)


costs["euclidean"] = cost


@pymanopt_function.jax(geometry["product"].data)
def cost(mu, sigma):
    point = (mu, sigma)
    moments = restriction["product"].g_bar(point)
    return jnp.dot(moments, moments)


costs["product"] = cost

results = {}

optimizer = TrustRegions(verbosity=0)
results = {
    manifold: optimizer.run(
        Problem(manifold=geometry[manifold].data, cost=costs[manifold])
    )
    for manifold in ["euclidean", "product"]
}
#+end_src

** Evaluate
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
for manifold in ["euclidean", "product"]:
    mu_hat, sigma_hat = results[manifold].point
    residual_norm = jnp.linalg.norm(
        restriction[manifold].g_bar(results[manifold].point)
    )

    print(f"** Manifold : {manifold} **", end="\n\n")
    print("Estimated mean:", np.asarray(mu_hat))
    print("Estimated covariance:\n", np.asarray(sigma_hat))
    print("Moment residual norm:", float(residual_norm))
    print("True mean:", np.asarray(mu))
    print("True covariance:\n", Sigma)
    print()

#+end_src


