#+TITLE: GMM on a Product Manifold: Gaussian Location & Covariance
#+AUTHOR: ManifoldGMM Maintainers
#+OPTIONS: toc:nil num:nil
#+PROPERTY: header-args:python :exports code :noweb yes

* Motivation

Ordinary GMM estimation of multivariate Gaussian models treats the location
parameter in Euclidean space and the covariance matrix in the cone of symmetric
positive definite matrices.  When moments are estimated naively, sampling noise
can produce covariance estimates that are indefinite or violate the manifold
structure.  Instead, we place the parameters on a *product manifold*
\[
\mathbb{R}^2 \times \mathcal{S}_{++}^{2},
\]
where \(\mathbb{R}^2\) captures the mean vector and \(\mathcal{S}_{++}^{2}\) denotes the space of \(2\times 2\)
PSD matrices with the canonical affine-invariant metric.

This example sketches how to express the corresponding GMM problem using
~MomentRestriction~ with a JAX backend, and optimise it via the product manifold
support available in =pymanopt=.

* Data and Moments

Let \(x_i \in \mathbb{R}^2\) be i.i.d. draws from a centred Gaussian with true mean
\(\mu_\star\) and covariance \(\Sigma_\star\).  

The first moments (2) are equal to \(\mu_*\), the second (3), because of symmetry) equal to \(\Sigma_*\), and the third equal to zero.
We stack the per-observation moments as
\[
g_i(\mu, \Sigma) =\begin{bmatrix}
x_i - \mu \\
\operatorname{vec}_s\big((x_i - \mu)(x_i - \mu)^{\top} - \Sigma\big)\\
(x_i-\mu)\otimes(x_i-\mu)\otimes(x_i-\mu)
\end{bmatrix}
\]
where \(\operatorname{vec}_s(\cdot)\) vectorizes the symmetric matrix using the
upper-triangular entries.  The moment restriction is satisfied when
\(\mathbb{E}[g_i(\mu_\star, \Sigma_\star)] = 0\).

* Product Manifold Structure

The optimization takes place on the product manifold

- ~Euclidean(2)~ for the mean \(\mu\)
- ~SymmetricPositiveDefinite(2)~ (SPD) for the covariance \(\Sigma\)

=pymanopt= exposes this manifold via ~Product((Euclidean(2), SPD(2)))~.
~MomentRestriction~ can accept the same product manifold by instantiating both
components separately and returning a ~ManifoldPoint~ that wraps the pair
~(\mu, \Sigma)~.

#+name: gaussian-imports
#+begin_src python :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
"""Gaussian location/covariance GMM example (tangled from docs)."""

from __future__ import annotations

from itertools import combinations_with_replacement

import jax.numpy as jnp
from jax.flatten_util import ravel_pytree
import numpy as np
from datamat import DataMat, DataVec
from manifoldgmm import GMM, GMMResult, Manifold, MomentRestriction
from pymanopt.manifolds import Euclidean, Product, SymmetricPositiveDefinite

#+end_src

* DataMat-first Moment Restriction

Users can implement per-observation moments purely with =DataMat= objects.
~MomentRestriction.from_datamat~ converts these moments into a backend-friendly
function (NumPy or JAX) while preserving the richer metadata for inspection.

** Data Generation
For this experiment we want some data drawn from a multivariate normal distribution.  We'll pretend we don't know the mean or variance, and estimate those.   After estimation we'll peek behind the curtain and compare our estimates with the true values.

#+name: gaussian-data
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance

# Default random variables are distributed standard normal
X = DataMat.random((512, 2), rng=67, columns=["x1", "x2"], colnames="p", idxnames="i")

S = DataMat(
    [[1.0, 0.4], [0.4, 1.1]], index=X.columns, columns=X.columns
)  # Sqrt of variance matrix

mu = DataVec([0.3, -0.2], index=X.columns)

X = X @ S + mu  # Transformed random deviates

Sigma = S @ S.T
#+end_src
** Defining Moment Restriction
We need to define a function which satisfies \(\mathbb{E}g_i(\theta)=0\).  And to use automatic differentiation we need a "jax" version.   The function ``gi`` shows the observation-level logic using pure DataVec/DataMat operations; for the JAX backend we supply ``gi_jax`` so
pymanopt can differentiate the objective automatically. (Future work may bridge the two automatically as described in [[file:../design/feature_requests.org][feature_requests.org]].)

#+name: gaussian-moments
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
useful_3rd_moments = list(combinations_with_replacement([0, 1], 3))


def gi(theta: tuple[DataVec, DataMat], x: DataVec) -> DataVec:
    mu, sigma = theta
    e = x - mu
    e2 = e.outer(e) - sigma
    e3 = e.kron(e).kron(e)  # 3rd central moments = 0
    return e.concat([e2.triu().vec, e3.loc[useful_3rd_moments]])


def gi_jax(theta: tuple[jnp.ndarray, jnp.ndarray], x: jnp.ndarray) -> jnp.ndarray:
    mu, sigma = theta
    e = x - mu
    e2 = e[:, None] @ e[None, :] - sigma
    e3 = jnp.kron(e, jnp.kron(e, e))

    dim = x.shape[0]
    triple_idx = jnp.array(useful_3rd_moments)
    flatten_idx = triple_idx @ jnp.array([dim * dim, dim, 1])

    return jnp.concatenate([e, e2[jnp.triu_indices(dim)], e3[flatten_idx]])

#+end_src

Internally ~restriction~ builds a JAX-compatible representation for optimization,
while methods such as ~g_bar~ and ~omega_hat~ still return =DataMat= objects with
the original labels. Note that :class:`MomentRestriction` rescales each stacked
moment by the observed sample size (√N) so the quadratic form
``g_bar(θ)' W g_bar(θ)`` is already a χ² statistic under the null, even with
missing data—no extra ``× N`` factor should be applied later.

We next build a ~MomentRestriction~ object which describes a function of parameters which is equal to zero in expected value (this is the =gi= or =gi_jax= function above).  We're interested in evaluating this function on a particular manifold.  So: below we define a manifold =geometry=, and build =restriction=.

#+name: gaussian-momentrestriction
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance

geometry = {}
restriction = {}


geometry["euclidean"] = Manifold.from_pymanopt(Product((Euclidean(2), Euclidean(2, 2))))

restriction["euclidean"] = MomentRestriction(
    gi_jax=gi_jax, data=X.to_jax().values, manifold=geometry["euclidean"], backend="jax"
)

geometry["product"] = Manifold.from_pymanopt(
    Product((Euclidean(2), SymmetricPositiveDefinite(2)))
)

restriction["product"] = MomentRestriction(
    gi_jax=gi_jax, data=X.to_jax().values, manifold=geometry["product"], backend="jax"
) 

#+end_src

#+name: gaussian-inference-helpers
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
def jacobian_dense(operator, basis) -> np.ndarray:
    """Assemble a dense Jacobian matrix given a list of tangent directions."""

    columns: list[np.ndarray] = []
    for direction in basis:
        image = operator.matvec(direction)
        flat_image, _ = ravel_pytree(image)
        columns.append(np.asarray(flat_image, dtype=float))
    return np.vstack(columns).T


def sandwich_covariance(D: np.ndarray, W: np.ndarray, S: np.ndarray) -> np.ndarray:
    """Compute the GMM sandwich covariance (D' W D)^-1 D' W S W D (D' W D)^-1."""

    G = D.T @ W @ D
    G_inv = np.linalg.inv(G)
    middle = D.T @ W @ S @ W @ D
    return G_inv @ middle @ G_inv


def parameter_labels_euclidean() -> list[str]:
    labels = [f"mu[{index}]" for index in range(2)]
    labels.extend([f"sigma[{i},{j}]" for i in range(2) for j in range(2)])
    return labels


def parameter_labels_product() -> list[str]:
    return ["mu[0]", "mu[1]", "sigma[0,0]", "sigma[0,1]", "sigma[1,1]"]


** Estimation
The `GMM` class wraps `MomentRestriction`, manages the weighting matrix, and
invokes the appropriate `pymanopt` optimiser (TrustRegions by default). We
create one estimator per manifold, supply an initial point in the ambient
coordinates, and call `.estimate()` to run the continuously updated procedure.

#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
cue_results = {}

cue_results["euclidean"] = GMM(
    restriction["euclidean"],
    initial_point=(jnp.zeros(2), jnp.zeros((2, 2))),
).estimate()

cue_results["product"] = GMM(
    restriction["product"],
    initial_point=(jnp.zeros(2), jnp.eye(2)),
).estimate()
#+end_src

** Evaluate
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
for manifold in ["euclidean", "product"]:
    result = cue_results[manifold]
    mu_hat, sigma_hat = result.theta

    print(f"** Manifold : {manifold} **", end="\n\n")
    print("Estimated mean:", np.asarray(mu_hat))
    print("Estimated covariance:\n", np.asarray(sigma_hat))
    print("J statistic (chi-squared):", result.criterion_value)
    print("Degrees of freedom:", result.degrees_of_freedom)
    print("True mean:", np.asarray(mu))
    print("True covariance:\n", Sigma)
    print()

#+end_src

* Inference

The estimator returns \(x_\star = (\mu_\star, \Sigma_\star)\) together with
\(\bar g_N(x_\star) \approx 0\).  To quantify uncertainty in \(x_\star\) (for
confidence regions or Wald tests) we need the Jacobian of the averaged moments
and the covariance of the sample moments.

** Euclidean baseline

Treat \(\theta = (\mu, \operatorname{vec}(\Sigma)) \in \mathbb{R}^6\) as a coordinate vector in the  ambient Euclidean space, and define \(D_\theta = \partial \bar g_N(\theta) / \partial \theta\) at the
estimate. The usual GMM sandwich covariance is
\[
\Sigma_\theta = (D_\theta^\top W D_\theta)^{-1} \left(D_\theta^\top W S W D_\theta\right)
           (D_\theta^\top W D_\theta)^{-1},
\]
where \(S = \Omega(x_\star)\) is the covariance of \(g_i(x_\star)\) and \(W\) is
the chosen weighting.  A standard result that if \(W\) is optimally chosen to be equal to \( S^{-1}\)), then 
\[
\Sigma_\theta = (D_\theta^\top S^{-1} D_\theta)^{-1},
\]
where \(S = \Omega(x_\star)\) and \(W\) is the chosen weighting
\((W = S^{-1}\) for the CUE estimator). The code below asks
=MomentRestriction.tangent_basis= for a canonical basis,
assembles \(D_\theta\) via =jacobian_dense=, and prints the resulting standard
errors.

#+name: gaussian-inference-euclidean
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
theta_euclidean = cue_results["euclidean"].theta
jacobian_euclidean = restriction["euclidean"].jacobian(theta_euclidean)
basis_euclidean = restriction["euclidean"].tangent_basis(theta_euclidean)
D_euclidean = jacobian_dense(jacobian_euclidean, basis_euclidean)
S_euclidean = np.asarray(restriction["euclidean"].omega_hat(theta_euclidean), dtype=float)
W_euclidean = np.linalg.inv(S_euclidean)
covariance_euclidean = sandwich_covariance(D_euclidean, W_euclidean, S_euclidean)
standard_errors_euclidean = np.sqrt(np.diag(covariance_euclidean))

print("Euclidean parameter standard errors")
for label, value in zip(parameter_labels_euclidean(), standard_errors_euclidean):
    print(f"{label:>12}: {value: .6f}")

#+end_src

** Manifold-aware inference

When \(\Sigma\) is constrained to lie on \(\mathcal{S}_{++}^2\), only the symmetric directions belong to the tangent space. The tangent-space Jacobian
\(D_{x_\star}\) acts on vectors \(\xi \in T_{x_\star}\mathcal{M}\) and the same sandwich formula applies with \(D = D_{x_\star}\). We span \(T_{x_\star}\mathcal{M}\) using Euclidean directions for \(\mu\) and symmetric directions for \(\Sigma\).  

The helper below applies =jacobian_dense= with =tangent_basis_product= to obtain covariances that live in the tangent coordinates of the product manifold.

#+name: gaussian-inference-product
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
Em = cue_results['product']



jacobian = restriction["product"].jacobian(theta)
basis = tangent_basis()
D = jacobian_dense(jacobian, basis)
S = np.asarray(restriction["product"].omega_hat(theta), dtype=float)
W = np.linalg.inv(S)
covariance = sandwich_covariance(D, W, S)
standard_errors = np.sqrt(np.diag(covariance))

print("\nManifold parameter standard errors")
for label, value in zip(parameter_labels_product(), standard_errors_product):
    print(f"{label:>12}: {value: .6f}")

#+end_src
