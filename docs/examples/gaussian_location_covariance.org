#+TITLE: GMM on a Product Manifold: Gaussian Location & Covariance
#+AUTHOR: ManifoldGMM Maintainers
#+OPTIONS: toc:nil num:nil
#+PROPERTY: header-args:python :exports code :noweb yes

* Motivation

Ordinary GMM estimation of multivariate Gaussian models treats the location
parameter in Euclidean space and the covariance matrix in the cone of symmetric
positive definite matrices.  When moments are estimated naively, sampling noise
can produce covariance estimates that are indefinite or violate the manifold
structure.  Instead, we place the parameters on a *product manifold*
\[
\mathbb{R}^2 \times \mathcal{S}_{++}^{2},
\]
where \(\mathbb{R}^2\) captures the mean vector and \(\mathcal{S}_{++}^{2}\) denotes the space of \(2\times 2\)
PSD matrices with the canonical affine-invariant metric.

This example sketches how to express the corresponding GMM problem using
~MomentRestriction~ with a JAX backend, and optimise it via the product manifold
support available in =pymanopt=.

* Data and Moments

Let \(x_i \in \mathbb{R}^2\) be i.i.d. draws from a centred Gaussian with true mean
\(\mu_\star\) and covariance \(\Sigma_\star\).  

The first moments (2) are equal to \(\mu_*\), the second (3), because of symmetry) equal to \(\Sigma_*\), and the third equal to zero.
We stack the per-observation moments as
\[
g_i(\mu, \Sigma) =\begin{bmatrix}
x_i - \mu \\
\operatorname{vec}_s\big((x_i - \mu)(x_i - \mu)^{\top} - \Sigma\big)\\
(x_i-\mu)\otimes(x_i-\mu)\otimes(x_i-\mu)
\end{bmatrix}
\]
where \(\operatorname{vec}_s(\cdot)\) vectorizes the symmetric matrix using the
upper-triangular entries.  The moment restriction is satisfied when
\(\mathbb{E}[g_i(\mu_\star, \Sigma_\star)] = 0\).

* Product Manifold Structure

The optimization takes place on the product manifold

- ~Euclidean(2)~ for the mean \(\mu\)
- ~SymmetricPositiveDefinite(2)~ (SPD) for the covariance \(\Sigma\)

=pymanopt= exposes this manifold via ~Product((Euclidean(2), SPD(2)))~.
~MomentRestriction~ can accept the same product manifold by instantiating both
components separately and returning a ~ManifoldPoint~ that wraps the pair
~(\mu, \Sigma)~.

#+name: gaussian-imports
#+begin_src python :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
"""Gaussian location/covariance GMM example (tangled from docs)."""

from __future__ import annotations

from itertools import combinations_with_replacement

import jax.numpy as jnp
from jax.flatten_util import ravel_pytree
import numpy as np
from datamat import DataMat, DataVec
from manifoldgmm import GMM, GMMResult, Manifold, MomentRestriction
from pymanopt.manifolds import Euclidean, Product, SymmetricPositiveDefinite

#+end_src

* DataMat-first Moment Restriction

Users can implement per-observation moments purely with =DataMat= objects.
~MomentRestriction.from_datamat~ converts these moments into a backend-friendly
function (NumPy or JAX) while preserving the richer metadata for inspection.

** Data Generation
For this experiment we want some data drawn from a multivariate normal distribution.  We'll pretend we don't know the mean or variance, and estimate those.   After estimation we'll peek behind the curtain and compare our estimates with the true values.

#+name: gaussian-data
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance

# Default random variables are distributed standard normal
X = DataMat.random((512, 2), rng=67, columns=["x1", "x2"], colnames="p", idxnames="i")

S = DataMat(
    [[1.0, 0.4], [0.4, 1.1]], index=X.columns, columns=X.columns
)  # Sqrt of variance matrix

mu = DataVec([0.3, -0.2], index=X.columns)

X = X @ S + mu  # Transformed random deviates

Sigma = S @ S.T
#+end_src
** Defining Moment Restriction
We need to define a function which satisfies \(\mathbb{E}g_i(\theta)=0\).  And to use automatic differentiation we need a "jax" version.   The function ``gi`` shows the observation-level logic using pure DataVec/DataMat operations; for the JAX backend we supply ``gi_jax`` so
pymanopt can differentiate the objective automatically. (Future work may bridge the two automatically as described in [[file:../design/feature_requests.org][feature_requests.org]].)

#+name: gaussian-moments
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
useful_3rd_moments = list(combinations_with_replacement([0, 1], 3))


def gi(theta: tuple[DataVec, DataMat], x: DataVec) -> DataVec:
    mu, sigma = theta
    e = x - mu
    e2 = e.outer(e) - sigma
    e3 = e.kron(e).kron(e)  # 3rd central moments = 0
    return e.concat([e2.triu().vec, e3.loc[useful_3rd_moments]])


def gi_jax(theta: tuple[jnp.ndarray, jnp.ndarray], x: jnp.ndarray) -> jnp.ndarray:
    mu, sigma = theta
    e = x - mu
    e2 = e[:, None] @ e[None, :] - sigma
    e3 = jnp.kron(e, jnp.kron(e, e))

    dim = x.shape[0]
    triple_idx = jnp.array(useful_3rd_moments)
    flatten_idx = triple_idx @ jnp.array([dim * dim, dim, 1])

    return jnp.concatenate([e, e2[jnp.triu_indices(dim)], e3[flatten_idx]])

#+end_src

Internally ~restriction~ builds a JAX-compatible representation for optimization,
while methods such as ~g_bar~ and ~omega_hat~ still return =DataMat= objects with
the original labels. Note that :class:`MomentRestriction` rescales each stacked
moment by the observed sample size (√N) so the quadratic form
``g_bar(θ)' W g_bar(θ)`` is already a χ² statistic under the null, even with
missing data—no extra ``× N`` factor should be applied later.

We next build a ~MomentRestriction~ object which describes a function of parameters which is equal to zero in expected value (this is the =gi= or =gi_jax= function above).  We're interested in evaluating this function on a particular manifold.  So: below we define a manifold =geometry=, and build =restriction=.

#+name: gaussian-momentrestriction
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance

geometry = {}
restriction = {}


geometry["euclidean"] = Manifold.from_pymanopt(Product((Euclidean(2), Euclidean(2, 2))))

restriction["euclidean"] = MomentRestriction(
    gi_jax=gi_jax, data=X.to_jax().values, manifold=geometry["euclidean"], backend="jax"
)

geometry["product"] = Manifold.from_pymanopt(
    Product((Euclidean(2), SymmetricPositiveDefinite(2)))
)

restriction["product"] = MomentRestriction(
    gi_jax=gi_jax, data=X.to_jax().values, manifold=geometry["product"], backend="jax"
) 

#+end_src

#+name: gaussian-inference-helpers
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
def jacobian_dense(operator, basis) -> np.ndarray:
    """Assemble a dense Jacobian matrix given a list of tangent directions."""

    columns: list[np.ndarray] = []
    for direction in basis:
        image = operator.matvec(direction)
        flat_image, _ = ravel_pytree(image)
        columns.append(np.asarray(flat_image, dtype=float))
    return np.vstack(columns).T


def sandwich_covariance(D: np.ndarray, W: np.ndarray, S: np.ndarray) -> np.ndarray:
    """Compute the GMM sandwich covariance (D' W D)^-1 D' W S W D (D' W D)^-1."""

    G = D.T @ W @ D
    G_inv = np.linalg.inv(G)
    middle = D.T @ W @ S @ W @ D
    return G_inv @ middle @ G_inv


def parameter_labels_euclidean() -> list[str]:
    labels = [f"mu[{index}]" for index in range(2)]
    labels.extend([f"sigma[{i},{j}]" for i in range(2) for j in range(2)])
    return labels


def parameter_labels_product() -> list[str]:
    return ["mu[0]", "mu[1]", "sigma[0,0]", "sigma[0,1]", "sigma[1,1]"]


** Estimation
The `GMM` class wraps `MomentRestriction`, manages the weighting matrix, and
invokes the appropriate `pymanopt` optimiser (TrustRegions by default). We
create one estimator per manifold, supply an initial point in the ambient
coordinates, and call `.estimate()` to run the continuously updated procedure.

#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
cue_results = {}

cue_results["euclidean"] = GMM(
    restriction["euclidean"],
    initial_point=(jnp.zeros(2), jnp.zeros((2, 2))),
).estimate()

cue_results["product"] = GMM(
    restriction["product"],
    initial_point=(jnp.zeros(2), jnp.eye(2)),
).estimate()
#+end_src

** Evaluate
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
for manifold, result in cue_results.items():
    theta_point = result.theta  # ManifoldPoint with formatted view cached lazily
    mu_hat, sigma_hat = theta_point.components()

    print(f"** Manifold : {manifold} **", end="\n\n")
    print("Estimated mean:", np.asarray(mu_hat))
    print("Estimated covariance:\n", np.asarray(sigma_hat))
    print("J statistic (chi-squared):", result.criterion_value)
    print("Degrees of freedom:", result.degrees_of_freedom)
    print("Raw parameter (manifold-aware):", theta_point.value)
    print("Formatted parameter:", result.theta_labeled)
    print("True mean:", np.asarray(mu))
    print("True covariance:\n", Sigma)
    print()

#+end_src

* Inference

The updated =GMMResult= exposes =tangent_covariance()= and =manifold_covariance()= helpers that assemble the sandwich estimator automatically. The former lives in the canonical tangent basis that spans \(T_{x_\star}\mathcal{M}\); the latter pushes the covariance into the ambient coordinates defined by the manifold adapter.

#+name: gaussian-inference-updated
#+begin_src python :exports code :tangle ../../examples/gaussian_covariance.py :session gaussian_covariance
for manifold, result in cue_results.items():
    tangent_cov = result.tangent_covariance()
    ambient_cov = result.manifold_covariance()

    tangent_se = np.sqrt(tangent_cov.dg().values)
    ambient_se = np.sqrt(ambient_cov.dg().values)

    print(f"== {manifold.title()} manifold inference ==")
    print("Tangent-space covariance:\n", tangent_cov)
    print("Tangent-space s.e.:", tangent_se)
    print("Ambient covariance:\n", ambient_cov)
    print("Ambient s.e.:", ambient_se)
    print()

#+end_src
